<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>IFT6266 Blog</title>
    <description>Blog post on the advencement of my deep learning project for the course ift6266.
</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 30 Apr 2017 16:01:35 -0400</pubDate>
    <lastBuildDate>Sun, 30 Apr 2017 16:01:35 -0400</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title></title>
        <description>
&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;It is difficult to evaluate this model, because there is no clear metric to measure the “realness” of images.&lt;/p&gt;

&lt;p&gt;Here some of the results I get:
&lt;img src=&quot;/home/louis/code/ift6266/docs/static_files/random.jpg&quot; alt=&quot;alt text&quot; title=&quot;Results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I have tried to generate with false captions, to see if the generator have learnt to use them :
&lt;img src=&quot;/home/louis/code/ift6266/docs/static_files/wrong_captions.jpg&quot; alt=&quot;alt text&quot; title=&quot;Wrong captions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We not see some clear difference. I think the conditioning on the caption is not significant in this generator model. However, the loss &lt;code class=&quot;highlighter-rouge&quot;&gt;Ld_fake_captions&lt;/code&gt;
go to zero on the discriminator, that mean that the discriminator predict the reject class (0) for images from the dataset with an invalid caption. The discriminator is then able to use the information contained in the embeddings.
Then the generator inability to
In future work, I should see to a better conditioning on the caption in the generator.&lt;/p&gt;

&lt;p&gt;But we can instead experiment a variation on the conditioning to observe the dependance the model have learn on it.&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Apr 2017 16:01:35 -0400</pubDate>
        <link>/2017/04/30/2.html</link>
        <guid isPermaLink="true">/2017/04/30/2.html</guid>
        
        
      </item>
    
      <item>
        <title></title>
        <description>&lt;h1 id=&quot;conditional-gan&quot;&gt;Conditional GAN&lt;/h1&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;
&lt;p&gt;Here a schema of the generator architecture :
&lt;img src=&quot;/home/louis/code/ift6266/docs/static_files/Archi gen.jpg&quot; alt=&quot;generator&quot; title=&quot;archi generator&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The different models are defined in &lt;a href=&quot;https://github.com/ogrergo/ift6266/blob/master/model.py&quot;&gt;model.py&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I choose to work with a DCGAN architecture to try to learn the image manifold. Using GAN as a generative model have the advantage of producing sharp images, unlike variational auto-encoder for example.&lt;/p&gt;

&lt;p&gt;I wanted to learn the &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;tensorflow library&lt;/a&gt;.
This library is actively maintained by Google, and is on the way to be a good buisness solution for market product deep learning application.&lt;/p&gt;

&lt;p&gt;I started with a working implementation of a DCGAN that I found on &lt;a href=&quot;https://github.com/carpedm20/DCGAN-tensorflow&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To test it on our task, I first trained it to generate full &lt;em&gt;64x64&lt;/em&gt; images without conditioning. I didn’t get good results, the MSCOCO images having great variance, the models didn’t show promising results.&lt;/p&gt;

&lt;h3 id=&quot;conditioning&quot;&gt;Conditioning&lt;/h3&gt;
&lt;p&gt;#### Border&lt;/p&gt;

&lt;p&gt;The first condition is the border. The network must learn to integrate the border information as :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a scene description, the border must give hints on the content and the structure of the image.&lt;/li&gt;
  &lt;li&gt;a continuity constraint on the reconstructed &lt;em&gt;64x64&lt;/em&gt; images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I use convolution on the border with skip connection to the same size deconvolution layer, where I concatenate the channels.&lt;/p&gt;

&lt;p&gt;The output of the convolution layers is flatten and concatenate to the initial noise and the captions embeddings.&lt;/p&gt;

&lt;p&gt;The discriminator is not conditioned on the border, instead, the border and the center are merged before entering the discriminator.&lt;/p&gt;

&lt;h4 id=&quot;captions&quot;&gt;Captions&lt;/h4&gt;

&lt;p&gt;The captions are first preprocessed into embeddings into a &lt;em&gt;1024&lt;/em&gt; dims vector space. I have used a model I found on &lt;a href=&quot;https://github.com/ryankiros/visual-semantic-embedding&quot;&gt;github&lt;/a&gt; trained on the same dataset, MSCOCO. In this model, images and sentences are mapped into a common vector space, where the sentence representation is computed using LSTM.&lt;/p&gt;

&lt;p&gt;The captions embeddings are concatenate with the noise vector in the generator, and in the discriminator, it is concatenate just before the MLP layers.&lt;/p&gt;

&lt;h2 id=&quot;training&quot;&gt;Training&lt;/h2&gt;

&lt;h3 id=&quot;discriminator-loss&quot;&gt;Discriminator loss&lt;/h3&gt;
&lt;p&gt;Here the loss of the discriminator:
&lt;code class=&quot;highlighter-rouge&quot;&gt;Ld = 0.5*Ld_real + 0.25*(Ld_fake_captions + Ld_generator_imgs)&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Ld_real&lt;/code&gt; is the loss of predicting the accept class for image taken from the training set and a random linear combination of his associated captions. I have add some label smoothing (try to predict 0.9 +/-0.1).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Ld_fake_captions&lt;/code&gt; is the loss of predicting the refuse class for image taken from the training with a random caption embedding.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Ld_generator_imgs&lt;/code&gt; is the loss of predicting the refuse class for image generated by the generator.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;generator-loss&quot;&gt;Generator loss&lt;/h3&gt;

&lt;p&gt;The generator is trained to fool the discriminator, to predict the accept class with his generated images.&lt;/p&gt;

&lt;h3 id=&quot;gpu&quot;&gt;GPU&lt;/h3&gt;
&lt;p&gt;I have use Amazon instance to train my models. I used p2.xlarge instance, which are equipped with Kepler M80 GPU. I trained this model for 10 epoch for 2 days.&lt;/p&gt;

&lt;h3 id=&quot;training-set&quot;&gt;Training set&lt;/h3&gt;
&lt;p&gt;The model is train on a training set made of 110000 examples. I have merged the training set and 99% of the validation set. I have done that because I don’t have a performance metric, so I will evaluate the model with my eyes. So I can do it with only 4000 examples.&lt;/p&gt;

&lt;p&gt;Each example is made of:
- An image of &lt;em&gt;64x64&lt;/em&gt; pixels with three colours channels
- A list of captions embeddings of variable dimension &lt;em&gt;(n, 1024)&lt;/em&gt; where &lt;em&gt;n&lt;/em&gt; is the number of captions associated with the image (5).&lt;/p&gt;

&lt;p&gt;I have build an asynchronous input pipeline using tensorflow &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Examples&lt;/code&gt; binary format, to leverage all the import code in the computation graph.
You can find the dataset
This give two major advantage :
- The dataset is then a set of 60 tfrecords files, each files containing 4096 examples. The dataset is space efficient and fairly simple to manipulate.
- The extraction of the examples can be defined directly in the computation graph, with a queue system. The examples are fed in the queues by threads.&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Apr 2017 16:01:35 -0400</pubDate>
        <link>/2017/04/30/1.html</link>
        <guid isPermaLink="true">/2017/04/30/1.html</guid>
        
        
      </item>
    
      <item>
        <title></title>
        <description>
</description>
        <pubDate>Sun, 30 Apr 2017 16:01:35 -0400</pubDate>
        <link>/2017/04/30/0.html</link>
        <guid isPermaLink="true">/2017/04/30/0.html</guid>
        
        
      </item>
    
  </channel>
</rss>
