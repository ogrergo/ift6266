<h1 id="conditional-gan">Conditional GAN</h1>

<h2 id="model">Model</h2>
<p>Here a schema of the generator architecture :
<img src="/home/louis/code/ift6266/docs/static_files/Archi gen.jpg" alt="generator" title="archi generator" /></p>

<p>The different models are defined in <a href="https://github.com/ogrergo/ift6266/blob/master/model.py">model.py</a>.</p>

<p>I choose to work with a DCGAN architecture to try to learn the image manifold. Using GAN as a generative model have the advantage of producing sharp images, unlike variational auto-encoder for example.</p>

<p>I wanted to learn the <a href="https://www.tensorflow.org/">tensorflow library</a>.
This library is actively maintained by Google, and is on the way to be a good buisness solution for market product deep learning application.</p>

<p>I started with a working implementation of a DCGAN that I found on <a href="https://github.com/carpedm20/DCGAN-tensorflow">github</a>.</p>

<p>To test it on our task, I first trained it to generate full <em>64x64</em> images without conditioning. I didn’t get good results, the MSCOCO images having great variance, the models didn’t show promising results.</p>

<h3 id="conditioning">Conditioning</h3>
<p>#### Border</p>

<p>The first condition is the border. The network must learn to integrate the border information as :</p>

<ul>
  <li>a scene description, the border must give hints on the content and the structure of the image.</li>
  <li>a continuity constraint on the reconstructed <em>64x64</em> images.</li>
</ul>

<p>I use convolution on the border with skip connection to the same size deconvolution layer, where I concatenate the channels.</p>

<p>The output of the convolution layers is flatten and concatenate to the initial noise and the captions embeddings.</p>

<p>The discriminator is not conditioned on the border, instead, the border and the center are merged before entering the discriminator.</p>

<h4 id="captions">Captions</h4>

<p>The captions are first preprocessed into embeddings into a <em>1024</em> dims vector space. I have used a model I found on <a href="https://github.com/ryankiros/visual-semantic-embedding">github</a> trained on the same dataset, MSCOCO. In this model, images and sentences are mapped into a common vector space, where the sentence representation is computed using LSTM.</p>

<p>The captions embeddings are concatenate with the noise vector in the generator, and in the discriminator, it is concatenate just before the MLP layers.</p>

<h2 id="training">Training</h2>

<h3 id="discriminator-loss">Discriminator loss</h3>
<p>Here the loss of the discriminator:
<code class="highlighter-rouge">Ld = 0.5*Ld_real + 0.25*(Ld_fake_captions + Ld_generator_imgs)</code></p>

<ul>
  <li><code class="highlighter-rouge">Ld_real</code> is the loss of predicting the accept class for image taken from the training set and a random linear combination of his associated captions. I have add some label smoothing (try to predict 0.9 +/-0.1).</li>
  <li><code class="highlighter-rouge">Ld_fake_captions</code> is the loss of predicting the refuse class for image taken from the training with a random caption embedding.</li>
  <li><code class="highlighter-rouge">Ld_generator_imgs</code> is the loss of predicting the refuse class for image generated by the generator.</li>
</ul>

<h3 id="generator-loss">Generator loss</h3>

<p>The generator is trained to fool the discriminator, to predict the accept class with his generated images.</p>

<h3 id="gpu">GPU</h3>
<p>I have use Amazon instance to train my models. I used p2.xlarge instance, which are equipped with Kepler M80 GPU. I trained this model for 10 epoch for 2 days.</p>

<h3 id="training-set">Training set</h3>
<p>The model is train on a training set made of 110000 examples. I have merged the training set and 99% of the validation set. I have done that because I don’t have a performance metric, so I will evaluate the model with my eyes. So I can do it with only 4000 examples.</p>

<p>Each example is made of:
- An image of <em>64x64</em> pixels with three colours channels
- A list of captions embeddings of variable dimension <em>(n, 1024)</em> where <em>n</em> is the number of captions associated with the image (5).</p>

<p>I have build an asynchronous input pipeline using tensorflow <code class="highlighter-rouge">tf.Examples</code> binary format, to leverage all the import code in the computation graph.
You can find the dataset
This give two major advantage :
- The dataset is then a set of 60 tfrecords files, each files containing 4096 examples. The dataset is space efficient and fairly simple to manipulate.
- The extraction of the examples can be defined directly in the computation graph, with a queue system. The examples are fed in the queues by threads.</p>
